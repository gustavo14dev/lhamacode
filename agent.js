import { MemorySystem } from './memory-system.js';

export class Agent {
    constructor(ui) {
        this.ui = ui;
        this.groqApiKey = null;
        this.currentModel = 'raciocinio';
        this.groqUrl = 'https://api.groq.com/openai/v1/chat/completions';
        this.conversationHistory = [];
        this.maxHistoryMessages = 50;
        this.abortController = null;
        this.isGenerating = false;
        
        // Sistema de mem√≥ria
        this.memory = new MemorySystem();
        this.memory.loadFromLocalStorage();
    }

    setModel(model) {
        this.currentModel = model;
    }

    getGroqApiKey() {
        if (!this.groqApiKey) {
            this.groqApiKey = localStorage.getItem('groq_api_key');
        }
        return this.groqApiKey;
    }

    // Verifica√ß√£o r√°pida de API antes de processar
    async quickApiCheck() {
        const apiKey = this.getGroqApiKey();
        
        if (!apiKey) {
            throw new Error('Nenhuma API key configurada');
        }
        
        // Fazer uma requisi√ß√£o r√°pida para testar a API
        try {
            const response = await fetch(this.groqUrl, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${apiKey}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    model: 'llama-3.1-8b-instant',
                    messages: [{ role: 'user', content: 'test' }],
                    max_tokens: 1
                })
            });
            
            if (!response.ok) {
                throw new Error(`API retornou status ${response.status}`);
            }
            
            return true;
        } catch (error) {
            console.error('‚ùå Verifica√ß√£o r√°pida da API falhou:', error);
            throw error;
        }
    }

    async processMessage(userMessage, attachedFilesFromUI = null) {
        console.log('üì® Mensagem para processar:', userMessage.substring(0, 100) + '...');
        console.log('üì® Tamanho total:', userMessage.length, 'caracteres');

        // Adicionar mensagem √† mem√≥ria da conversa
        this.memory.addConversationMemory('user', userMessage);

        // Obter contexto relevante da mem√≥ria
        const relevantContext = this.memory.getRelevantContext(userMessage);
        console.log('üß† Contexto relevante encontrado:', relevantContext.length, 'mem√≥rias');

        // Verifica√ß√£o r√°pida da API desabilitada temporariamente
        // try {
        //     await this.quickApiCheck();
        //     console.log('‚úÖ API verificada e funcionando');
        // } catch (error) {
        //     console.error('‚ùå Falha na verifica√ß√£o da API:', error);
        //     
        //     // Mostrar card de erro e agendar retry
        //     if (typeof window.scheduleApiRetry === 'function') {
        //         window.scheduleApiRetry(userMessage, attachedFilesFromUI);
        //     } else {
        //         // Fallback se a fun√ß√£o n√£o estiver dispon√≠vel
        //         this.ui.addAssistantMessage('‚ùå Erro na API. Por favor, configure sua API key.');
        //     }
        //     return;
        // }

        // Se a UI passou arquivos explicitamente, priorizamos esses (m√°x 3)
        let parsedFiles = [];
        if (attachedFilesFromUI && Array.isArray(attachedFilesFromUI) && attachedFilesFromUI.length > 0) {
            parsedFiles = attachedFilesFromUI.slice(0, 3).map(f => ({ name: f.name, content: (f.content == null) ? '' : String(f.content) }));
            console.log('üìÅ Arquivos recebidos diretamente da UI:', parsedFiles.map(f => `${f.name} (${(f.content||'').length} chars)`));
            const emptyFiles = parsedFiles.filter(f => !f.content || f.content.trim().length === 0);
            if (emptyFiles.length > 0) {
                const names = emptyFiles.map(f => f.name).join(', ');
                const warning = `‚ùó Alguns arquivos anexados est√£o vazios ou n√£o foram salvos corretamente: ${names}. Por favor, verifique os arquivos.`;
                console.warn(warning);
                this.ui.addAssistantMessage(warning);
                return; // Bloquear processamento
            }
            // Preparar blocos para envio ao modelo
            this.lastParsedFiles = parsedFiles;
            this.extraMessagesForNextCall = [{ role: 'system', content: parsedFiles.map(f => `---FILE: ${f.name}---\n${f.content}\n---END FILE---`).join('\n\n') }];
            console.log('‚û°Ô∏è Arquivos anexados preparados para envio:', parsedFiles.map(f => f.name).join(', '));
            this.useMistralForThisMessage = true;
        } else {
            this.lastParsedFiles = [];
            this.extraMessagesForNextCall = null;
            this.useMistralForThisMessage = false;

            // Se n√£o h√° anexos do usu√°rio, verificar se o chat tem arquivos gerados anteriormente pelo assistente (para reutiliza√ß√£o)
            try {
                const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
                if (chat && chat.generatedFiles && chat.generatedFiles.length > 0) {
                    this.lastParsedFiles = chat.generatedFiles.slice(0, 3).map(f => ({ name: f.name, content: f.content }));
                    this.extraMessagesForNextCall = [{ role: 'system', content: this.lastParsedFiles.map(f => `---FILE: ${f.name}---\n${f.content}\n---END FILE---`).join('\n\n') }];
                    console.log('‚ôªÔ∏è Reusando arquivos gerados pelo assistente do chat para pr√≥xima chamada:', this.lastParsedFiles.map(f => f.name));
                }
            } catch (e) {
                console.warn('‚ö†Ô∏è Erro verificando arquivos gerados do chat:', e);
            }
        }

        this.isGenerating = true;
        this.ui.updateSendButtonToPause();
        
        if (this.currentModel === 'rapido') {
            await this.processRapidoModel(userMessage, relevantContext);
        } else if (this.currentModel === 'raciocinio') {
            if (this.useMistralForThisMessage) {
                await this.processMistralModel(userMessage, relevantContext);
            } else {
                await this.processRaciocioModel(userMessage, relevantContext);
            }
        } else if (this.currentModel === 'pro') {
            await this.processProModel(userMessage, relevantContext);
        }
        
        this.isGenerating = false;
        this.ui.updateSendButtonToSend();
    }
    // ==================== MODELO MISTRAL (codestral-latest) ====================
    async processMistralModel(userMessage, relevantContext = []) {
        // Usamos proxy server-side; n√£o √© obrigat√≥rio ter chave no localStorage para o deploy no Vercel
        const messageContainer = this.ui.createAssistantMessageContainer();
        const timestamp = Date.now();
        this.ui.setThinkingHeader('Processando com Mistral (codestral-latest)...', messageContainer.headerId);
        await this.ui.sleep(800);
        this.addToHistory('user', userMessage);
        try {
            // Gerar checks antes da chamada Mistral para mostrar racioc√≠nio tamb√©m neste fluxo
            const thinkingChecks = await this.generateChecksSafely(userMessage);
            for (let i = 0; i < thinkingChecks.length; i++) {
                const stepId = `step_${timestamp}_${i}`;
                const checkText = thinkingChecks[i].step;
                this.ui.addThinkingStep('schedule', checkText, stepId, messageContainer.stepsId);
                const delay = 800 + Math.random() * 1200;
                await this.ui.sleep(delay);
                this.ui.updateThinkingStep(stepId, 'check_circle', checkText);
                await this.ui.sleep(200);
            }

            // Construir prompt com contexto da mem√≥ria
            let memoryContext = '';
            if (relevantContext.length > 0) {
                memoryContext = '\n\nCONTEXTO RELEVANTE DA CONVERSA:\n';
                relevantContext.forEach((memory, index) => {
                    memoryContext += `${index + 1}. ${memory.role.toUpperCase()}: "${memory.content}" (Contexto: ${memory.context})\n`;
                });
                memoryContext += '\nUse este contexto para fornecer respostas mais personalizadas e relevantes.';
            }
            
            let systemPrompt = {
                role: 'system',
                content: `Voc√™ √© o Drekee AI 1, um assistente de c√≥digo inteligente com mem√≥ria contextual. Forne√ßa respostas COMPLETAS e ESTRUTURADAS com: m√∫ltiplos par√°grafos bem organizados, **palavras em negrito** para destacar conceitos, listas com ‚Ä¢ ou n√∫meros, t√≥picos claros com headings, e quando apropriado use tabelas (em formato markdown), nota√ß√£o matem√°tica (com $s√≠mbolos$ para inline ou $$blocos$$), e diagramas em ASCII. Evite blocos enormes de c√≥digo - prefira explica√ß√µes visuais. Seja t√©cnico mas acess√≠vel.${memoryContext}`
            };
            const messages = this.extraMessagesForNextCall ? [systemPrompt, ...this.extraMessagesForNextCall, ...this.conversationHistory] : [systemPrompt, ...this.conversationHistory];

            // Chamamos o proxy server-side para Mistral (usar MISTRAL_API_KEY no servidor)
            let response = await this.callMistralAPI('codestral-latest', messages);
            this.extraMessagesForNextCall = null;

            if (!response || typeof response !== 'string') {
                throw new Error('Resposta vazia ou inv√°lida do servidor Mistral');
            }

            // Tentar extrair arquivos gerados na resposta e anex√°-los ao chat
            try {
                const parsedFiles = this.parseFilesFromText(response);
                if (parsedFiles && parsedFiles.length > 0) {
                    this.attachGeneratedFilesToChat(parsedFiles);
                    // Remover o bloco de arquivos do texto antes de exibir para usu√°rio
                    response = response.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) {
                console.warn('‚ö†Ô∏è Falha parsing arquivos de resposta Mistral:', e);
            }

            // Armazenar e salvar a mensagem do assistente (incluindo attachments, se houver) ANTES de renderizar para que o UI possa detect√°-los
            const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
            if (chat) {
                if (chat.messages.length === 1) {
                    const firstUserMessage = chat.messages[0].content;
                    chat.title = firstUserMessage.substring(0, 50) + (firstUserMessage.length > 50 ? '...' : '');
                }
                const toPush = { role: 'assistant', content: response, thinking: null };
                const parsedFiles = this.parseFilesFromText(response);
                if (parsedFiles && parsedFiles.length > 0) toPush.attachments = parsedFiles;
                chat.messages.push(toPush);
                this.ui.saveCurrentChat();
            }

            this.addToHistory('assistant', response);
            
            // Adicionar resposta da IA √† mem√≥ria e aprender com a intera√ß√£o
            this.memory.addConversationMemory('assistant', response);
            this.memory.learnFromInteraction(userMessage, response);
            
            this.ui.setResponseText(response, messageContainer.responseId);
            await this.ui.sleep(500);
            this.ui.closeThinkingSteps(messageContainer.headerId);
            
            // Gerar sugest√µes de acompanhamento
            await this.generateFollowUpSuggestions(userMessage, response, messageContainer.responseId);
        } catch (error) {
            if (error.message === 'ABORTED') {
                console.log('‚ö†Ô∏è Gera√ß√£o interrompida pelo usu√°rio');
                return;
            }
            this.ui.setResponseText('Desculpe, ocorreu um erro ao processar sua mensagem na API Mistral. ' + error.message, messageContainer.responseId);
            console.error('Erro no Modelo Mistral:', error);
        }
    }

    async callMistralAPI(model, messages) {
        // Usa unified-proxy server-side /api/unified-proxy
        this.abortController = new AbortController();
        try {
            const response = await fetch('/api/unified-proxy', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    service: 'mistral',
                    model: model,
                    messages: messages,
                    temperature: 0.7,
                    max_tokens: 2048
                }),
                signal: this.abortController.signal
            });

            if (!response.ok) {
                const status = response.status;
                const text = await response.text().catch(() => null);
                if (status === 500 && text && text.includes('MISTRAL_API_KEY is not configured')) {
                    throw new Error('Mistral API Key n√£o est√° configurada no servidor. Adicione MISTRAL_API_KEY nas Environment Variables do Vercel.');
                }
                if (status === 401) {
                    throw new Error('Invalid API Key Mistral: verifique sua chave no Vercel para MISTRAL_API_KEY');
                }
                throw new Error(text || `Erro HTTP ${status}`);
            }

            const data = await response.json();
            return data.choices[0].message.content;
        } catch (error) {
            if (error.name === 'AbortError') {
                throw new Error('ABORTED');
            }
            throw error;
        }
    }

    stopGeneration() {
        console.log('üõë Parando gera√ß√£o...');
        this.isGenerating = false;
        if (this.abortController) {
            this.abortController.abort();
        }
        this.ui.showInterruptedMessage();
        this.ui.updateSendButtonToSend();
    }

    // Tenta extrair bloco ---FILES-JSON--- ... ---END-FILES-JSON--- e retornar array de arquivos
    parseFilesFromText(text) {
        try {
            const m = text.match(/---FILES-JSON---\s*([\s\S]*?)\s*---END-FILES-JSON---/i);
            if (!m) return null;
            const parsed = JSON.parse(m[1]);
            if (parsed && Array.isArray(parsed.files)) return parsed.files;
            return null;
        } catch (e) {
            console.warn('‚ö†Ô∏è Falha ao parsear blocos de arquivos:', e);
            return null;
        }
    }

    // Gera checks chamando Groq com toler√¢ncia a falhas
    async generateChecksSafely(userMessage) {
        try {
            const checksResponse = await this.callGroqAPI('llama-3.1-8b-instant', [
                {
                    role: 'system',
                    content: 'Voc√™ √© um gerador de checklist de pensamento. Baseado na pergunta/tarefa do usu√°rio, gere de 3 a 10 etapas de pensamento que uma IA deveria fazer para responder bem. Retorne APENAS um JSON array com objetos {step: "texto da etapa"}. Exemplo: [{"step": "Analisando a pergunta"}, {"step": "Consultando dados"}]'
                },
                {
                    role: 'user',
                    content: `Gere os passos de pensamento para esta tarefa: "${userMessage.substring(0, 200)}"`
                }
            ]);

            // Tentar parse tolerante
            let jsonText = null;
            const arrayMatch = checksResponse.match(/\[[\s\S]*?\]/);
            const fencedJsonMatch = checksResponse.match(/```json\s*([\s\S]*?)```/i) || checksResponse.match(/```\s*([\s\S]*?)```/);
            if (arrayMatch) {
                jsonText = arrayMatch[0];
            } else if (fencedJsonMatch) {
                jsonText = fencedJsonMatch[1];
            }

            if (jsonText) {
                return JSON.parse(jsonText);
            }

            const lines = checksResponse.split(/\r?\n/).map(l => l.trim()).filter(Boolean);
            const listItems = lines.filter(l => /^(\-|\*|\d+\.)\s+/.test(l)).map(l => {
                const s = l.replace(/^(\-|\*|\d+\.)\s+/, '');
                return { step: s };
            });
            if (listItems.length > 0) return listItems;

            // fallback
            return [
                { step: 'Analisando a pergunta' },
                { step: 'Consultando modelo Llama 3' },
                { step: 'Processando dados' },
                { step: 'Estruturando resposta' }
            ];
        } catch (e) {
            console.warn('‚ö†Ô∏è Erro ao gerar checks, usando padr√£o:', e);
            return [
                { step: 'Analisando a pergunta' },
                { step: 'Consultando modelo Llama 3' },
                { step: 'Processando dados' },
                { step: 'Estruturando resposta' }
            ];
        }
    }

    // Anexa arquivos parseados ao objeto de chat para reutiliza√ß√£o em chamadas futuras
    attachGeneratedFilesToChat(files) {
        try {
            const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
            if (!chat) return;
            chat.generatedFiles = chat.generatedFiles || [];
            // substituir arquivos com mesmo nome
            files.forEach(f => {
                const idx = chat.generatedFiles.findIndex(x => x.name === f.name);
                if (idx >= 0) chat.generatedFiles[idx] = f; else chat.generatedFiles.push(f);
            });
            this.ui.saveCurrentChat();
        } catch (e) {
            console.warn('‚ö†Ô∏è Falha ao anexar arquivos ao chat:', e);
        }
    }
    // parseFilesFromMessage removed (attachment parsing disabled)

    // ==================== MODELO R√ÅPIDO ====================
    async processRapidoModel(userMessage) {
        // Usamos proxy server-side (/api/groq-proxy) que utiliza GROQ_API_KEY em Vercel.
        // N√£o √© necess√°rio ter chave no localStorage para deploy em produ√ß√£o.

        const messageContainer = this.ui.createRapidMessageContainer();
        const timestamp = Date.now();

        // Adicionar texto simples de carregamento com pontinhos pulsando
        const thinkingHeader = document.getElementById(`thinkingHeader_${messageContainer}`);
        if (thinkingHeader) {
            thinkingHeader.innerHTML = '<span class="inline-flex gap-1"><span class="w-2 h-2 bg-gray-400 rounded-full animate-pulse"></span><span class="w-2 h-2 bg-gray-400 rounded-full animate-pulse" style="animation-delay: 0.2s"></span><span class="w-2 h-2 bg-gray-400 rounded-full animate-pulse" style="animation-delay: 0.4s"></span></span>';
            thinkingHeader.className = 'text-base leading-relaxed text-gray-500 dark:text-gray-400';
        }

        this.addToHistory('user', userMessage);

        try {
            const messages = this.extraMessagesForNextCall ? [
                { role: 'system', content: this.getSystemPrompt('rapido') },
                ...this.extraMessagesForNextCall,
                ...this.conversationHistory
            ] : undefined;
            let response = await this.callGroqAPI('llama-3.1-8b-instant', messages);
            // limpar extras para pr√≥xima chamada
            this.extraMessagesForNextCall = null;

            this.addToHistory('assistant', response);
            this.ui.setResponseText(response, `responseText_${messageContainer}`);
            
            // Mostrar bot√µes de a√ß√£o quando resposta estiver completa
            const actionsDiv = document.getElementById(`actions_${messageContainer}`);
            if (actionsDiv) {
                actionsDiv.classList.remove('opacity-0');
                actionsDiv.classList.add('opacity-60', 'hover:opacity-100');
            }
            
            // Limpar texto de carregamento ap√≥s um pequeno delay
            setTimeout(() => {
                if (thinkingHeader) {
                    thinkingHeader.textContent = '';
                }
            }, 100);
            
            // Gerar sugest√µes de acompanhamento
            await this.generateFollowUpSuggestions(userMessage, response, `responseText_${messageContainer}`);

            const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
            if (chat) {
                if (chat.messages.length === 1) {
                    const firstUserMessage = chat.messages[0].content;
                    chat.title = firstUserMessage.substring(0, 50) + (firstUserMessage.length > 50 ? '...' : '');
                }
                chat.messages.push({ role: 'assistant', content: response, thinking: null });
                this.ui.saveCurrentChat();
            }

        } catch (error) {
            if (error.message === 'ABORTED') {
                console.log('‚ö†Ô∏è Gera√ß√£o interrompida pelo usu√°rio');
                return;
            }
            this.ui.setResponseText('Desculpe, ocorreu um erro ao processar sua mensagem. ' + error.message, `responseText_${messageContainer}`);
            console.error('Erro no Modelo R√°pido:', error);
        }
    }

    // ==================== MODELO RACIOC√çNIO ====================
    async processRaciocioModel(userMessage) {
        const messageContainer = this.ui.createAssistantMessageContainer();
        const timestamp = Date.now();

        this.ui.setThinkingHeader('üß≠ Processando racioc√≠nio...', messageContainer.headerId);
        await this.ui.sleep(800);

        this.addToHistory('user', userMessage);

        try {
            // √öNICA CHAMADA API com modelo de racioc√≠nio
            const systemPrompt = { 
                role: 'system', 
                content: this.getSystemPrompt('raciocinio') + 
                ' Voc√™ √© um modelo de racioc√≠nio. Pense passo a passo sobre a pergunta do usu√°rio e coloque seu racioc√≠nio completo dentro de tags <think>...</think>. Depois do racioc√≠nio, forne√ßa a resposta final.'
            };
            
            const messages = this.extraMessagesForNextCall ? 
                [systemPrompt, ...this.extraMessagesForNextCall, ...this.conversationHistory] : 
                [systemPrompt, ...this.conversationHistory];
            
            console.log('üß≠ Usando modelo de racioc√≠nio: qwen/qwen3-32b');
            let fullResponse = await this.callGroqAPI('qwen/qwen3-32b', messages);
            this.extraMessagesForNextCall = null;
            
            // Extrair racioc√≠nio e resposta
            let reasoningText = '';
            let finalResponse = fullResponse;
            
            // Procurar por tags <think>...</think>
            const thinkMatch = fullResponse.match(/<think>([\s\S]*?)<\/think>/);
            if (thinkMatch) {
                reasoningText = thinkMatch[1].trim();
                finalResponse = fullResponse.replace(/<think>[\s\S]*?<\/think>/, '').trim();
            }
            
            // Tentar extrair arquivos gerados na resposta e anex√°-los ao chat
            try {
                const parsedFiles = this.parseFilesFromText(finalResponse);
                if (parsedFiles && parsedFiles.length > 0) {
                    this.attachGeneratedFilesToChat(parsedFiles);
                    finalResponse = finalResponse.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) {
                console.warn('‚ö†Ô∏è Falha parsing arquivos de resposta (Racioc√≠nio):', e);
            }

            this.addToHistory('assistant', finalResponse);
            
            // Mostrar resposta final
            this.ui.setResponseText(finalResponse, messageContainer.responseId);
            
            // Limpar header de processamento
            this.ui.setThinkingHeader('', messageContainer.headerId);
            
            // Mostrar bot√£o "Mostrar racioc√≠nio" se houver racioc√≠nio
            if (reasoningText) {
                const showBtn = document.getElementById(messageContainer.showId);
                if (showBtn) {
                    showBtn.classList.remove('hidden');
                    showBtn.innerHTML = `
                        <span class="material-icons-outlined text-sm">expand_more</span>
                        Mostrar racioc√≠nio
                    `;
                    
                    // Adicionar evento para mostrar/ocultar racioc√≠nio
                    showBtn.onclick = () => {
                        const stepsDiv = document.getElementById(messageContainer.stepsId);
                        if (stepsDiv) {
                            if (stepsDiv.classList.contains('hidden')) {
                                // Mostrar racioc√≠nio
                                stepsDiv.classList.remove('hidden');
                                stepsDiv.innerHTML = `
                                    <div class="bg-gray-50 dark:bg-gray-800 rounded-lg p-4 border border-gray-200 dark:border-gray-700">
                                        <div class="text-sm text-gray-600 dark:text-gray-400 whitespace-pre-wrap">${reasoningText}</div>
                                    </div>
                                `;
                                showBtn.innerHTML = `
                                    <span class="material-icons-outlined text-sm">expand_less</span>
                                    Ocultar racioc√≠nio
                                `;
                            } else {
                                // Ocultar racioc√≠nio
                                stepsDiv.classList.add('hidden');
                                showBtn.innerHTML = `
                                    <span class="material-icons-outlined text-sm">expand_more</span>
                                    Mostrar racioc√≠nio
                                `;
                            }
                        }
                    };
                }
            }
            
            // Mostrar bot√µes de a√ß√£o quando resposta estiver completa
            const actionsDiv = document.getElementById(`actions_${messageContainer.container.id.replace('msg_', '')}`);
            if (actionsDiv) {
                actionsDiv.classList.remove('opacity-0');
                actionsDiv.classList.add('opacity-60', 'hover:opacity-100');
            }
            
            // Gerar sugest√µes de acompanhamento
            await this.generateFollowUpSuggestions(userMessage, finalResponse, messageContainer.responseId);

        } catch (error) {
            if (error.message === 'ABORTED') {
                console.log('‚ö†Ô∏è Gera√ß√£o interrompida pelo usu√°rio');
                return;
            }
            this.ui.setResponseText('Desculpe, ocorreu um erro ao processar sua mensagem. Verifique sua API Key e tente novamente.', messageContainer.responseId);
            console.error('Erro no Modelo Racioc√≠nio:', error);
        }
    }

    // ==================== MODELO PRO ====================
// 3x llama-3.1-8b-instant (an√°lise 1 ‚Üí an√°lise 2 ‚Üí s√≠ntese)
    async processProModel(userMessage) {
        const messageContainer = this.ui.createAssistantMessageContainer();
        const timestamp = Date.now();

        this.ui.setThinkingHeader('üöÄ An√°lise multi-perspectivas...', messageContainer.headerId);
        await this.ui.sleep(800);

        this.addToHistory('user', userMessage);

        try {
            // ========== ETAPA 1: Primeira an√°lise ==========
            const step1Text = 'Analisando perspectiva 1...';
            this.ui.setThinkingHeader(step1Text, messageContainer.headerId);
            await this.ui.sleep(2000);
            this.ui.setThinkingHeader('', messageContainer.headerId);
            await this.ui.sleep(500);
            
            const messages1 = this.extraMessagesForNextCall ? [
                { role: 'system', content: this.getSystemPrompt('pro') },
                ...this.extraMessagesForNextCall,
                ...this.conversationHistory,
                { role: 'user', content: userMessage }
            ] : [
                { role: 'system', content: this.getSystemPrompt('pro') },
                ...this.conversationHistory,
                { role: 'user', content: userMessage }
            ];
            
            const response1 = await this.callGroqAPI('llama-3.1-8b-instant', messages1);
            
            // Extrair arquivos se existirem
            try {
                const parsed1 = this.parseFilesFromText(response1);
                if (parsed1 && parsed1.length > 0) {
                    this.attachGeneratedFilesToChat(parsed1);
                    response1 = response1.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) { console.warn('‚ö†Ô∏è Falha parsing arquivos de response1:', e); }

            // ========== ETAPA 2: Segunda an√°lise ==========
            const step2Text = 'Analisando perspectiva 2...';
            this.ui.setThinkingHeader(step2Text, messageContainer.headerId);
            await this.ui.sleep(2000);
            this.ui.setThinkingHeader('', messageContainer.headerId);
            await this.ui.sleep(500);
            
            const messages2 = this.extraMessagesForNextCall ? [
                { role: 'system', content: this.getSystemPrompt('pro') },
                ...this.extraMessagesForNextCall,
                ...this.conversationHistory,
                { role: 'user', content: userMessage }
            ] : [
                { role: 'system', content: this.getSystemPrompt('pro') },
                ...this.conversationHistory,
                { role: 'user', content: userMessage }
            ];
            
            const response2 = await this.callGroqAPI('llama-3.1-8b-instant', messages2);
            
            // Extrair arquivos se existirem
            try {
                const parsed2 = this.parseFilesFromText(response2);
                if (parsed2 && parsed2.length > 0) {
                    this.attachGeneratedFilesToChat(parsed2);
                    response2 = response2.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) { console.warn('‚ö†Ô∏è Falha parsing arquivos de response2:', e); }

            // ========== ETAPA 3: S√≠ntese final ==========
            const step3Text = 'Sintetizando an√°lises...';
            this.ui.setThinkingHeader(step3Text, messageContainer.headerId);
            await this.ui.sleep(2000);
            this.ui.setThinkingHeader('', messageContainer.headerId);
            await this.ui.sleep(500);
            
            const synthMessages = [
                {
                    role: 'system',
                    content: this.getSystemPrompt('pro') + ' Voc√™ √© um especialista em s√≠ntese. Combine e melhore as duas respostas abaixo em uma √∫nica resposta superior. Corrija poss√≠veis erros, melhore a clareza, e crie uma resposta final otimizada.'
                },
                {
                    role: 'user',
                    content: `Pergunta original: "${userMessage}"

=== RESPOSTA 1 (Perspectiva 1) ===
${response1}

=== RESPOSTA 2 (Perspectiva 2) ===
${response2}

Combine e melhore as duas respostas em uma √∫nica resposta coesa e superior. Corrija poss√≠veis erros, melhore a clareza, e crie uma resposta final otimizada.`
                }
            ];
            
            // Se houver arquivos anexados, inclu√≠-los temporariamente nas mensagens de s√≠ntese
            if (this.extraMessagesForNextCall) {
                synthMessages.splice(1, 0, ...this.extraMessagesForNextCall);
            }
            
            let finalResponse = await this.callGroqAPI('llama-3.1-8b-instant', synthMessages);
            this.extraMessagesForNextCall = null;

            // Tentar extrair arquivos gerados na resposta final e anex√°-los ao chat
            try {
                const parsedFiles = this.parseFilesFromText(finalResponse);
                if (parsedFiles && parsedFiles.length > 0) {
                    this.attachGeneratedFilesToChat(parsedFiles);
                    finalResponse = finalResponse.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) {
                console.warn('‚ö†Ô∏è Falha parsing arquivos de resposta (Pro):', e);
            }

            this.addToHistory('assistant', finalResponse);
            this.ui.setResponseText(finalResponse, messageContainer.responseId);
            
            // Mostrar bot√µes de a√ß√£o quando resposta estiver completa
            const actionsDiv = document.getElementById(`actions_${messageContainer.container.id.replace('msg_', '')}`);
            if (actionsDiv) {
                actionsDiv.classList.remove('opacity-0');
                actionsDiv.classList.add('opacity-60', 'hover:opacity-100');
            }
            
            // Fechar racioc√≠nio quando terminar
            await this.ui.sleep(500);
            this.ui.closeThinkingSteps(messageContainer.headerId);
            
            // Gerar sugest√µes de acompanhamento
            await this.generateFollowUpSuggestions(userMessage, finalResponse, messageContainer.responseId);

            const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
            if (chat) {
                if (chat.messages.length === 1) {
                    const firstUserMessage = chat.messages[0].content;
                    chat.title = firstUserMessage.substring(0, 50) + (firstUserMessage.length > 50 ? '...' : '');
                }
                const toPush = { role: 'assistant', content: finalResponse, thinking: null };
                const parsedFiles = this.parseFilesFromText(finalResponse);
                if (parsedFiles && parsedFiles.length > 0) toPush.attachments = parsedFiles;
                chat.messages.push(toPush);
                this.ui.saveCurrentChat();
            }

        } catch (error) {
            if (error.message === 'ABORTED') {
                console.log('‚ö†Ô∏è Gera√ß√£o interrompida pelo usu√°rio');
                return;
            }
            this.ui.setResponseText('Desculpe, ocorreu um erro ao processar sua mensagem. Verifique sua API Key e tente novamente.', messageContainer.responseId);
            console.error('Erro no Modelo Pro:', error);
        }
    }

    // ==================== APIS ====================
    // Gemini API methods removed (attachments/Gemini integration disabled)

    async callGroqAPI(model, customMessages = null) {
        console.log('üöÄ callGroqAPI iniciado');
        console.log('üìã Modelo:', model);
        console.log('üìã Mensagens customizadas:', customMessages ? 'SIM' : 'N√ÉO');
        console.log('üìã Hist√≥rico atual:', this.conversationHistory.length, 'mensagens');
        
        // Not required to have a client-side Groq API key when using server-side proxy
        // The proxy will use GROQ_API_KEY from environment variables on Vercel
        
        // System prompts diferenciados por modelo
        let systemPrompt;
        if (this.currentModel === 'rapido') {
            systemPrompt = {
                role: 'system',
                content: 'Voc√™ √© o Drekee AI 1, um assistente de c√≥digo r√°pido e direto. Mantenha as respostas BREVES e CONCISAS - m√°ximo 2-3 par√°grafos. Evite elabora√ß√µes desnecess√°rias. V√° direto ao ponto.'
            };
        } else {
            // Racioc√≠nio e Pro - respostas ricas
            systemPrompt = {
                role: 'system',
                content: 'Voc√™ √© o Drekee AI 1, um assistente de c√≥digo inteligente. Forne√ßa respostas COMPLETAS e ESTRUTURADAS com: m√∫ltiplos par√°grafos bem organizados, **palavras em negrito** para destacar conceitos, listas com ‚Ä¢ ou n√∫meros, t√≥picos claros com headings, e quando apropriado use tabelas (em formato markdown), nota√ß√£o matem√°tica (com $s√≠mbolos$ para inline ou $$blocos$$), e diagramas em ASCII. Evite blocos enormes de c√≥digo - prefira explica√ß√µes visuais. Seja t√©cnico mas acess√≠vel.'
            };
        }

        const messages = customMessages || [systemPrompt, ...this.conversationHistory];
        
        console.log('üì§ Mensagens finais para API:', messages.length, 'mensagens');
        console.log('üì§ Primeira mensagem:', messages[0]?.content?.substring(0, 100) + '...');
        if (messages.length > 1) {
            console.log('üì§ √öltima mensagem:', messages[messages.length - 1]?.content?.substring(0, 100) + '...');
        }

        // Criar novo AbortController para cada requisi√ß√£o
        this.abortController = new AbortController();

        try {
            console.log('üì° Enviando requisi√ß√£o para /api/groq-proxy...');
            
            const requestBody = {
                model, 
                messages, 
                temperature: 0.7, 
                max_tokens: 8192, 
                top_p: 1, 
                stream: false
            };
            
            console.log('üì¶ Corpo da requisi√ß√£o:', requestBody);

            // Chamar proxy server-side no Vercel
            const response = await fetch('/api/groq-proxy', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(requestBody),
                signal: this.abortController.signal
            });

            console.log('üì° Resposta recebida:', response.status, response.statusText);
            console.log('üì° Headers:', Object.fromEntries(response.headers.entries()));

            if (!response.ok) {
                const status = response.status;
                const text = await response.text().catch(() => null);
                console.error('‚ùå Erro na resposta:', status, text);
                
                // Mensagens amig√°veis para erros comuns
                if (status === 500 && text && text.includes('GROQ_API_KEY is not configured')) {
                    throw new Error('GROQ API Key n√£o est√° configurada no servidor. Adicione GROQ_API_KEY nas Environment Variables do Vercel.');
                }
                if (status === 401) {
                    throw new Error('Invalid API Key: Verifique sua chave no Vercel para GROQ_API_KEY.');
                }
                throw new Error(text || `Erro HTTP ${status}`);
            }

            const data = await response.json().catch(() => ({}));
            console.log('üì¶ Dados recebidos:', data);

            // Normalizar formatos comuns de resposta de proxies/LLMs
            let content = null;
            if (typeof data.content === 'string') {
                content = data.content;
            } else if (data.choices && Array.isArray(data.choices) && data.choices[0]) {
                const choice = data.choices[0];
                if (choice.message && typeof choice.message.content === 'string') {
                    content = choice.message.content;
                } else if (typeof choice.text === 'string') {
                    content = choice.text;
                }
            } else if (typeof data === 'string') {
                content = data;
            }

            console.log('üìù Conte√∫do extra√≠do:', content ? content.substring(0, 200) + '...' : 'NULO');

            if (!content || typeof content !== 'string' || content.trim().length === 0) {
                console.error('[callGroqAPI] resposta inesperada do proxy:', data);
                throw new Error('Resposta vazia ou formato inesperado do proxy Groq');
            }

            console.log('‚úÖ callGroqAPI conclu√≠do com sucesso');
            return content;
        } catch (error) {
            console.error('‚ùå Erro em callGroqAPI:', error);
            if (error.name === 'AbortError') {
                console.log('‚ö†Ô∏è Requisi√ß√£o foi abortada pelo usu√°rio');
                throw new Error('ABORTED');
            }
            throw error;
        }
    }

    // ==================== UTILITIES ====================
    addToHistory(role, content) {
        this.conversationHistory.push({
            role: role,
            content: content
        });

        if (this.conversationHistory.length > this.maxHistoryMessages) {
            this.conversationHistory.shift();
            console.log('üóëÔ∏è Mensagem mais antiga removida (limite de 10 mensagens)');
        }
    }

    // Retorna o system prompt apropriado por 'mode' para estabelecer tom/estilo (inclui emojis)
    getSystemPrompt(mode) {
        switch (mode) {
            case 'rapido':
                return 'Voc√™ √© o Drekee AI 1, um assistente gentil, ador√°vel e otimista üòä. Use um tom caloroso e amig√°vel, inclua emojis com leveza para refor√ßar emo√ß√µes, e mantenha as respostas BREVES e objetivas (2-3 par√°grafos m√°ximo). Seja educado, encorajador e pr√°tico. Use formata√ß√£o livre: **negrito**, *it√°lico*, t√≠tulos, listas, etc.';
            case 'raciocinio':
                return 'Voc√™ √© o Drekee AI 1, um assistente t√©cnico e claro üôÇ. Use emojis de forma moderada para tornar o texto mais acess√≠vel. Forne√ßa respostas COMPLETAS e ESTRUTURADAS com exemplos e explica√ß√µes claras. Sinta-se LIVRE para usar: **negrito**, *it√°lico*, <u>sublinhado</u>, t√≠tulos (# ## ###), par√°grafos bem organizados, listas (‚Ä¢ ou n√∫meros), tabelas markdown, express√µes matem√°ticas LaTeX ($inline$ ou $$bloco$$), diagramas ASCII, e qualquer outro elemento que torne a resposta mais clara e profissional. Escolha criativamente o melhor formato para cada tipo de conte√∫do! **IMPORTANTE:** Quando pedir gr√°ficos, CRIE o gr√°fico visualmente usando ASCII art, barras com caracteres, ou elementos visuais - n√£o apenas descreva o gr√°fico. **MATEM√ÅTICA:** Use TODOS os s√≠mbolos matem√°ticos poss√≠veis: fra√ß√µes (1/2), equa√ß√µes LaTeX ($E=mc^2$), s√≠mbolos Unicode (Œ±, Œ≤, Œ≥, ‚àë, ‚à´, ‚àÇ, ‚àá, ¬±, √ó, √∑, ‚âà, ‚â†, ‚â§, ‚â•, ‚àû, ‚àö), letras gregas (Œ±, Œ≤, Œ≥, Œ¥, Œµ, Œ∏, Œª, Œº, œÄ, œÉ, œÑ, œÜ, œá, œà, œâ), conjuntos (‚àà, ‚àâ, ‚äÇ, ‚äÉ, ‚äÜ, ‚äá, ‚à™, ‚à©, ‚àÖ), l√≥gica (‚àÄ, ‚àÉ, ¬¨, ‚àß, ‚à®, ‚Üí, ‚Üê, ‚Üî, ‚áí, ‚áê, ‚áî), setas (‚Üí, ‚Üê, ‚Üî, ‚áí, ‚áê, ‚áî), operadores (‚äï, ‚äó, ‚äô, ‚ä•), graus (¬∞), primos (‚Ä≤, ‚Ä≥, ‚Ä¥), sobrescritos (^2, ^3) e subscritos (_1, _2). Renderize TUDO perfeitamente!';
            case 'pro':
                return 'Voc√™ √© o Drekee AI 1, um assistente profissional e formal üßë‚Äçüíº. Use linguagem precisa e formal; inclua emojis pontualmente para dar tom (com parcim√¥nia). Forne√ßa an√°lises detalhadas, recomenda√ß√µes e justificativas bem fundamentadas. Tenha TOTAL LIBERDADE criativa na formata√ß√£o: use **negrito estrat√©gico**, *it√°lico para √™nfase*, <u>sublinhado</u>, t√≠tulos hier√°rquicos, par√°grafos estruturados, listas numeradas e com marcadores, tabelas profissionais, express√µes matem√°ticas LaTeX ($f√≥rmulas$ e $$blocos$$), gr√°ficos ASCII, e qualquer elemento que melhore a comunica√ß√£o. Adapte o formato ao conte√∫do de forma inteligente! **IMPORTANTE:** Quando pedir gr√°ficos, CRIE o gr√°fico visualmente usando ASCII art, barras com caracteres, ou elementos visuais - n√£o apenas descreva o gr√°fico. **MATEM√ÅTICA:** Use TODOS os s√≠mbolos matem√°ticos poss√≠veis: fra√ß√µes (1/2), equa√ß√µes LaTeX ($E=mc^2$), s√≠mbolos Unicode (Œ±, Œ≤, Œ≥, ‚àë, ‚à´, ‚àÇ, ‚àá, ¬±, √ó, √∑, ‚âà, ‚â†, ‚â§, ‚â•, ‚àû, ‚àö), letras gregas (Œ±, Œ≤, Œ≥, Œ¥, Œµ, Œ∏, Œª, Œº, œÄ, œÉ, œÑ, œÜ, œá, œà, œâ), conjuntos (‚àà, ‚àâ, ‚äÇ, ‚äÉ, ‚äÜ, ‚äá, ‚à™, ‚à©, ‚àÖ), l√≥gica (‚àÄ, ‚àÉ, ¬¨, ‚àß, ‚à®, ‚Üí, ‚Üê, ‚Üî, ‚áí, ‚áê, ‚áî), setas (‚Üí, ‚Üê, ‚Üî, ‚áí, ‚áê, ‚áî), operadores (‚äï, ‚äó, ‚äô, ‚ä•), graus (¬∞), primos (‚Ä≤, ‚Ä≥, ‚Ä¥), sobrescritos (^2, ^3) e subscritos (_1, _2). Renderize TUDO perfeitamente!';
            default:
                return 'Voc√™ √© o Drekee AI 1, um assistente de c√≥digo. Forne√ßa respostas claras e √∫teis, com boa estrutura e exemplos quando adequado. Use formata√ß√£o rica: **negrito**, *it√°lico*, t√≠tulos, listas, tabelas, LaTeX, etc.';
        }
    }

    showError(message) {
        const messageContainer = this.ui.createAssistantMessageContainer();
        const timestamp = Date.now();
        
        const errorStepId = `errorStep_${timestamp}`;
        this.ui.addThinkingStep('error', 'Erro detectado', errorStepId, messageContainer.stepsId);
        
        this.ui.setResponseText(message, messageContainer.responseId);
        
        console.error(message);
    }

    async test() {
        console.log('üß™ Iniciando teste do agente...');
        
        console.log('üì° Testando conex√£o com Groq via proxy (server-side) ...');
        console.log('‚ÑπÔ∏è Se voc√™ configurou a vari√°vel GROQ_API_KEY no Vercel, este teste usar√° ela. Caso contr√°rio, o teste falhar√° com mensagem adequada.');

        try {
            const testMessage = 'Ol√°! Estou testando a conex√£o.';
            console.log(`üì§ Enviando: "${testMessage}"`);
            
            this.addToHistory('user', testMessage);
            const response = await this.callGroqAPI('llama-3.3-70b-versatile');
            this.addToHistory('assistant', response);
            
            console.log('‚úÖ Resposta recebida:');
            console.log(response);
            console.log('\nüéâ Teste conclu√≠do com sucesso!');
            console.log(`üìä Hist√≥rico: ${this.conversationHistory.length} mensagens`);
            
            return response;
        } catch (error) {
            console.error('‚ùå Erro no teste:', error.message);
            console.error('Detalhes:', error);
            return null;
        }
    }

    clearHistory() {
        this.conversationHistory = [];
        console.log('üóëÔ∏è Hist√≥rico de conversa limpo');
    }

    getHistoryStats() {
        const userMessages = this.conversationHistory.filter(m => m.role === 'user').length;
        const assistantMessages = this.conversationHistory.filter(m => m.role === 'assistant').length;
        
        console.log('üìä Estat√≠sticas do Hist√≥rico:');
        console.log(`   Total: ${this.conversationHistory.length} mensagens`);
        console.log(`   Suas mensagens: ${userMessages}`);
        console.log(`   Minhas respostas: ${assistantMessages}`);
        console.log(`   Limite m√°ximo: ${this.maxHistoryMessages} mensagens`);
        
        return {
            total: this.conversationHistory.length,
            user: userMessages,
            assistant: assistantMessages,
            max: this.maxHistoryMessages
        };
    }

    // Extra√ß√£o e retorno de arquivos removidos (download de arquivos pela IA desativado)

    async generateFollowUpSuggestions(userMessage, assistantResponse, responseId) {
        try {
            const prompt = `Voc√™ √© um assistente de IA. Baseado na conversa abaixo, gere EXATAMENTE 3 sugest√µes de pr√≥ximas perguntas que o USU√ÅRIO poderia fazer para voc√™. As sugest√µes devem ser:

- Na perspectiva do USU√ÅRIO falando com a IA
- Perguntas naturais e relevantes
- Baseadas no contexto da conversa
- Escritas como se o usu√°rio estivesse perguntando

Conversa:
Usu√°rio perguntou: "${userMessage}"
Voc√™ respondeu: "${assistantResponse.substring(0, 500)}..."

Exemplos de como devem ser:
- "Como funciona [t√≥pico mencionado]?"
- "Pode me explicar mais sobre [assunto]?"
- "O que voc√™ acha de [ideia relacionada]?"

Responda APENAS com um JSON array contendo 3 strings, sem texto adicional:
["pergunta do usu√°rio 1", "pergunta do usu√°rio 2", "pergunta do usu√°rio 3"]`;

            const response = await this.callGroqAPI('llama-3.1-8b-instant', [
                { role: 'system', content: 'Voc√™ √© um especialista em gerar sugest√µes de acompanhamento relevantes e naturais para conversas. Sempre retorne exatamente 3 sugest√µes em formato JSON array.' },
                { role: 'user', content: prompt }
            ]);

            // Extrair JSON da resposta
            let suggestions = [];
            try {
                // Limpar resposta e extrair JSON
                let cleanResponse = response.replace(/```json\n?|\n?```/g, '').trim();
                const startIdx = cleanResponse.indexOf('[');
                const endIdx = cleanResponse.lastIndexOf(']');
                
                if (startIdx !== -1 && endIdx !== -1) {
                    const jsonStr = cleanResponse.substring(startIdx, endIdx + 1);
                    suggestions = JSON.parse(jsonStr);
                }
                
                // Validar e filtrar sugest√µes
                if (Array.isArray(suggestions)) {
                    suggestions = suggestions
                        .filter(s => typeof s === 'string' && s.trim().length > 0)
                        .map(s => s.trim())
                        .slice(0, 3);
                }
                
                if (suggestions.length > 0) {
                    // Extrair ID da mensagem a partir do responseId
                    const messageId = responseId.replace('responseText_', 'msg_');
                    this.ui.displayFollowUpSuggestions(messageId, suggestions);
                    console.log('‚úÖ Sugest√µes de acompanhamento geradas:', suggestions);
                }
            } catch (parseError) {
                console.warn('‚ö†Ô∏è Erro ao parsear sugest√µes:', parseError);
            }
            
        } catch (error) {
            console.warn('‚ö†Ô∏è Erro ao gerar sugest√µes de acompanhamento:', error);
            // N√£o mostrar erro para usu√°rio, apenas log
        }
    }

    sleep(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
}
